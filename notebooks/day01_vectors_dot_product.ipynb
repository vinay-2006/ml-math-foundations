{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe18b5e",
   "metadata": {},
   "source": [
    "# Day 01 — Vectors and Dot Product\n",
    "\n",
    "This notebook establishes the mathematical foundation of machine learning by\n",
    "explaining vectors, dot products, and their role in model predictions.\n",
    "\n",
    "The focus is on understanding, not library usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544704a",
   "metadata": {},
   "source": [
    "## Section 1 — Manual Vectors (No NumPy)\n",
    "\n",
    "In machine learning, a **vector** represents a single data point.\n",
    "Each element of the vector corresponds to one feature.\n",
    "\n",
    "A **weight vector** represents how strongly each feature influences the prediction.\n",
    "\n",
    "Before using libraries, we work with vectors manually to understand their meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb21e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 3], [1, 4])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature vectors (pure Python, no NumPy)\n",
    "x1 = [2, 3]\n",
    "x2 = [1, 4]\n",
    "\n",
    "x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f08648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual vector addition\n",
    "vector_sum = [x1[0] + x2[0], x1[1] + x2[1]]\n",
    "vector_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b133d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar multiplication\n",
    "scalar = 2\n",
    "scaled_vector = [scalar * x1[0], scalar * x1[1]]\n",
    "scaled_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464a92e",
   "metadata": {},
   "source": [
    "### Interpretation in Machine Learning Terms\n",
    "\n",
    "• **Vector**  \n",
    "  A vector represents one data point.  \n",
    "  Each component is a feature (e.g., size, age, score).\n",
    "\n",
    "• **Vector Addition**  \n",
    "  Adding vectors can be interpreted as combining feature signals.\n",
    "  While not commonly used directly in models, it helps build intuition\n",
    "  for how feature values interact numerically.\n",
    "\n",
    "• **Scalar Multiplication**  \n",
    "  Scaling a vector increases or decreases the influence of all features.\n",
    "  In ML, this mirrors how increasing feature magnitude changes its impact\n",
    "  on predictions and gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d969e",
   "metadata": {},
   "source": [
    "## Section 2 — Dot Product (Manual)\n",
    "\n",
    "The **dot product** is the most important operation in machine learning.\n",
    "\n",
    "It measures how aligned an input vector is with a weight vector.\n",
    "This alignment produces a single scalar value that becomes the model's prediction\n",
    "(before adding bias).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69ac5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight vector\n",
    "w = [0.5, 1.0]\n",
    "\n",
    "# Input feature vector\n",
    "x = [2, 3]\n",
    "\n",
    "# Manual dot product\n",
    "dot_product = w[0] * x[0] + w[1] * x[1]\n",
    "dot_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334394e8",
   "metadata": {},
   "source": [
    "### Why This Equals Prediction\n",
    "\n",
    "Each feature is multiplied by its corresponding weight.\n",
    "This represents how important that feature is.\n",
    "\n",
    "The dot product sums these weighted features into a single value.\n",
    "\n",
    "This value is the model's **score**:\n",
    "• high score → strong alignment between input and weights  \n",
    "• low score → weak alignment  \n",
    "\n",
    "Linear regression, logistic regression, and neural networks all rely on\n",
    "this same core operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cf80f",
   "metadata": {},
   "source": [
    "## Section 3 — NumPy Verification\n",
    "\n",
    "NumPy is used here only to **confirm** the manual computation.\n",
    "Understanding must come before library usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17ae7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w_np = np.array([0.5, 1.0])\n",
    "x_np = np.array([2, 3])\n",
    "\n",
    "np.dot(w_np, x_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfc5b0",
   "metadata": {},
   "source": [
    "The NumPy result matches the manual computation, confirming correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7776743",
   "metadata": {},
   "source": [
    "## Section 4 — Feature Scaling Thought Experiment\n",
    "\n",
    "### Why does changing feature scale affect the dot product?\n",
    "\n",
    "The dot product is sensitive to magnitude.\n",
    "If one feature has much larger numerical values than others,\n",
    "it will dominate the dot product regardless of actual importance.\n",
    "\n",
    "This causes the model to prioritize scale instead of meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### Why does this affect learning rate later?\n",
    "\n",
    "Gradient descent updates are proportional to feature values.\n",
    "\n",
    "Large feature scales produce large gradients,\n",
    "which can cause unstable updates, oscillation, or divergence.\n",
    "\n",
    "This is why **feature scaling** is essential before training models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
